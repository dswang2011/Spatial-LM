RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 64, in _worker
    output = module(*input, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ubuntu/python_projects/Spatial-LM/src/LMs/SpatialLM.py", line 65, in forward
    outputs = self.spatial_lm(
  File "/home/ubuntu/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/models/layoutlmv3/modeling_layoutlmv3.py", line 981, in forward
    encoder_outputs = self.encoder(
  File "/home/ubuntu/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/models/layoutlmv3/modeling_layoutlmv3.py", line 655, in forward
    rel_pos = self._cal_1d_pos_emb(hidden_states, position_ids) if self.has_relative_attention_bias else None
  File "/home/ubuntu/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/models/layoutlmv3/modeling_layoutlmv3.py", line 611, in _cal_1d_pos_emb
    rel_pos = self.rel_pos_bias(rel_pos).permute(0, 3, 1, 2)
  File "/home/ubuntu/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/ubuntu/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`