
[COMMON]
seed =  88

multi_gpu_accelerator = True

# 1. set dataset, model
# dataset: rvlcdip, funsd_cord
dataset_name = funsd_cord_sorie
# docvqa to use: {1,2,3}
train_part = 1
# layoutlm, spatial_lm, roberta, graph_roberta
network_type = spatial_lm
mlm_probability = 0.15
# output_dir = layoutlmv3-cord

# 2. {mlm, npp}
task_type = mlm


# do not use {binary-label, multi-label, regression }, because you need specific task preparation info.
# 2.2 task usage
# graph_feature = True
# graph_vect_path = /home/ubuntu/python_projects/DocGraph4LM/src/tmp_dir/graphsage_docvqa4g_522066/
# BERT sequence can be sub-word sequence;  

# 3. set hyper parameters
batch_size = 2
epochs = 3
lr = 0.00005
patience = 10
dropout = 0.1
max_seq_len = 512

# hidden_size = 768
# hidden_dim = 100
# hidden_dim_1 = 64
# hidden_dim_2 = 32

# 4. continue train
# continue_train = True
# continue_with_model = /home/ubuntu/air/vrdu/models/csmodel_rvlcdip_initial/

layoutlm_dir = /home/ubuntu/air/vrdu/models/layoutlmv3.large
checkpoint_path = /home/ubuntu/python_projects/Spatial-LM/src/trained_path/large


# other less common parameters 
embedding_trainable = True

rvl_cdip_ds = /home/ubuntu/air/vrdu/datasets/rvl_HF_datasets/weighted_rvl1_dataset.hf


# layoutlm_large = /home/ubuntu/air/vrdu/models/layoutlmv1.large
# layoutlm_large = /home/ubuntu/air/vrdu/models/roberta.base.squad
# layoutlm_dir = /home/ubuntu/resources/layoutlmv3.base
# layoutlm_dir = /home/ubuntu/air/vrdu/models/layoutlmv3.docvqa
# layoutlm_dir = /home/ubuntu/air/vrdu/models/layoutlmv3.large

# graph_vect_path
# graph_vect_path = /home/ubuntu/python_projects/GraphVRDU/src/tmp_dir/graphsage_cord4g_KNN/
