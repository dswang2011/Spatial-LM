
[COMMON]
seed =  88

multi_gpu_accelerator = True

# 1. set dataset, model
# dataset: rvlcdip
dataset_name = rvlcdip
# docvqa train has 3 parts, indicate which part to use: {1,2,3}
train_part = 1
# layoutlm, graph_layoutlm, roberta, graph_roberta
network_type = csmodel
mask_ratio = 0.2
# output_dir = layoutlmv3-cord

# 2. {docvqa, token-classifier, link-binary, node-classify, neib-regression, direct-classify; neib-regression}
# task_type = docvqa
task_type = cspretrain
# task_type = token-classifier
# task_type = direct-classify
# task_type = neib-regression
# task_type = node-classify

# do not use {binary-label, multi-label, regression }, because you need specific task preparation info.
# 2.2 task usage
# graph_feature = True
# graph_vect_path = /home/ubuntu/python_projects/DocGraph4LM/src/tmp_dir/graphsage_docvqa4g_522066/
# BERT sequence can be sub-word sequence;  

# 3. set hyper parameters
batch_size = 4
epochs = 1
lr = 0.00005
patience = 10
dropout = 0.1
max_seq_len = 512

# hidden_size = 768
# hidden_dim = 100
# hidden_dim_1 = 64
# hidden_dim_2 = 32

# 4. continue train
# continue_train = True
# continue_with_model = /home/ubuntu/air/vrdu/models/csmodel_rvlcdip_initial/

csmodel = /home/ubuntu/air/vrdu/models/cs_layoutlm_v3

# other less common parameters 
embedding_trainable = True

# rvl_cdip = /home/ubuntu/air/vrdu/datasets/rvl_pretrain_datasets/2_bert.hf

layoutlm_large = /home/ubuntu/air/vrdu/models/layoutlmv1.large
# layoutlm_large = /home/ubuntu/air/vrdu/models/roberta.base.squad
# layoutlm_dir = /home/ubuntu/resources/layoutlmv3.base
# layoutlm_dir = /home/ubuntu/air/vrdu/models/layoutlmv3.docvqa
# layoutlm_dir = /home/ubuntu/air/vrdu/models/layoutlmv3.large

# graph_vect_path
# graph_vect_path = /home/ubuntu/python_projects/GraphVRDU/src/tmp_dir/graphsage_cord4g_KNN/
